{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neurhci.uhci import UHCI\n",
    "from neurhci.hierarchical import HCI\n",
    "\n",
    "from neurhci.marginal_utilities import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file contains an example on how to build, train, and explain a model.\n",
    "\n",
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a hierarchy as a dict {id of the parents: [ids of the children]}. All ids are integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchy = {-1:[0,1,2,5], 5:[3,4]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corresponds to a tree with:\n",
    "* root -1 (since it has no parent)\n",
    "* leaves 0,1,2,3 and 4 (since they have no child).\n",
    "* 5 is an intermediate node, which aggregates 3 and 4.\n",
    "* The root -1 aggregates leaves 0,1,2 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our UHCI model with the hierarchy provided. \n",
    "We precise that leaf 0 has a non-decreasing marginal utility, and leaf 2 has a non-increasing one.\n",
    "Leaves 1, 3 and 4 will have no marginal utility (i.e. an Indentity).\n",
    "\n",
    "The marginal utilities each have 15 sigmoids (i.e. hidden neurons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UHCI(hierarchy=hierarchy, types_of_leaves={0:NonDecreasing, 2:NonIncreasing}, nb_sigmoids=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one already has a pre-trained HCI and/or MarginalUtilitiesLayer, they can be used to initialize the model.\n",
    "\n",
    "Any coherent combination of the different types of inputs works. For instance, the different lines below all create a similar model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hci = HCI(hierarchy) # can be pretrained\n",
    "marginal_utilities = MarginalUtilitiesLayer(list_of_leaves=[0,1,2,3,4], types_of_leaves={0:NonDecreasing, 2:NonIncreasing}, nb_sigmoids=15)\n",
    "\n",
    "model2 = UHCI(hci=hci, types_of_leaves={0:NonDecreasing, 2:NonIncreasing}, nb_sigmoids=15)\n",
    "model3 = UHCI(hci=hci, marginal_utilities=marginal_utilities)\n",
    "model4 = UHCI(hierarchy=hierarchy, marginal_utilities=marginal_utilities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data\n",
    "\n",
    "Below, we initialize some data for our model to train from. We show that the model used to generate the data is recovered by the trained model.\n",
    "\n",
    "The ground truth model is a HCI with same hierarchy as model, but described explicitely to avoid any ambiguity.\n",
    "\n",
    "Its marginal utilities are $u_0(x_0)=x_0^3$ and $u_2(x_2) = 1-x^2$. $u_1$, $u_3$ and $u_4$ are all identities.\n",
    "\n",
    "The aggregation corresponds to the following function:\n",
    "* at the intermediate node 5: $u_5 = 0.4 u_3 + 0.1 u_4 + 0.5 min(u_3, u_4)$\n",
    "* at the root -1: $u_{-1} = 0.2 u_0 + 0.3 u_2 + 0.2 u_5 + 0.2 min(u_0, u_5) + 0.1 max(u_1, u_2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_model(data):\n",
    "    #applying marginal utilities:\n",
    "    data_after_utilities = data.clone()\n",
    "    data_after_utilities[:,0] = data_after_utilities[:,0]**3\n",
    "    data_after_utilities[:,2] = 1-data_after_utilities[:,2]**2\n",
    "    \n",
    "    #aggregating at node 5:\n",
    "    u5 = 0.4*data_after_utilities[:,3] + 0.1*data_after_utilities[:,4]\n",
    "    u5 += torch.min(data_after_utilities[:,(3,4)], axis=1).values*0.5\n",
    "\n",
    "    #aggregating at node -1\n",
    "    gtd = 0.2*data_after_utilities[:,0] + 0.3*data_after_utilities[:,2] + 0.2*u5\n",
    "    gtd += torch.min(torch.cat((data_after_utilities[:, 0:1], u5.unsqueeze(1)), axis=-1), axis=1).values*0.2\n",
    "    gtd += torch.max(data_after_utilities[:, (1,2)], axis=1).values*0.1\n",
    "    gtd = gtd.unsqueeze(1)\n",
    "    return(data_after_utilities, gtd)\n",
    "\n",
    "data_train = torch.rand((200,5)) #draw random data for training ; since the model is (very) small, we do not need a lot.\n",
    "data_test = torch.rand((1000,5)) #draw random data for testing\n",
    "\n",
    "train_after_utilities, labels_train = ground_truth_model(data_train) #compute the ground-truth labels for training data\n",
    "test_after_utilities, labels_test = ground_truth_model(data_test) #compute the ground-truth labels for testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We now train our model with a basic torch loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.02) # can be tuned for better results\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(2000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data_train)\n",
    "    training_loss = criterion(pred, labels_train)\n",
    "    training_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    if not epoch%100:\n",
    "        with torch.no_grad():\n",
    "            pred_test = model(data_test)\n",
    "            testing_loss = criterion(pred_test, labels_test)\n",
    "        print(f'Epoch {epoch}, training loss {training_loss}, testing loss {testing_loss}')\n",
    "with torch.no_grad():\n",
    "    pred_train = model(data_train)\n",
    "    training_loss = criterion(pred_train, labels_train)\n",
    "    pred_test = model(data_test)\n",
    "    testing_loss = criterion(pred_test, labels_test)\n",
    "print(f'Final training loss {training_loss}, Final testing loss {testing_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the marginal utilities\n",
    "\n",
    "We plot the marginal utilities and compare them to the ground truth functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0,1,100).unsqueeze(1)\n",
    "gt_utilities = {0:lambda x:x**3, 2:lambda x:1-x*x}\n",
    "for i in range(5):\n",
    "    plt.subplot(2,3,i+1)\n",
    "    ground_truth = lambda x:x\n",
    "    if i in gt_utilities:\n",
    "        ground_truth = gt_utilities[i]\n",
    "    plt.plot(x.detach().numpy(), ground_truth(x).detach().numpy(), color='blue', marker='+', markevery=10, label='Ground Truth')\n",
    "    plt.plot(x.detach().numpy(), model.marginal_utilities[i](x).detach().numpy(), color='red', marker='x', markevery=15, label=\"Learned\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the aggregators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that the weights fit the ground truth's.\n",
    "\n",
    "Recall that: $u_5 = 0.4 u_3 + 0.1 u_4 + 0.5 min(u_3, u_4)$\n",
    "\n",
    "The weights are ordered such that (with n the size of the aggregator):\n",
    "    * the nodes from 0 to $n-1$ weights correspond to the n leaves, in order\n",
    "    * the nodes from n to $n(n+1)/2-1$ correspond to the min between pairs, with pairs\n",
    "        following the lexicographic ordering: $(0,1),(0,2),(...),(0,n),(1,2),(1,3)...,(n-1,n)$\n",
    "    * the nodes from $n(n+1)/2$ to $n^2-1$ correspond to max between pairs, ordered in the same way\n",
    "\n",
    "We thus expect the weight vector for u5 to be:\n",
    "[0.4, 0.1, 0.5, 0.]\n",
    "\n",
    "Since these weights are not identifiable, we need to compute their (unique) equivalent canonical form (see thesis).\n",
    "\n",
    "Note that the aggregators are accessed through the string version of their identifier (imposed by the nn.ModuleDict class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cweights5 = model.HCI.CIs['5'].canonical_weights()\n",
    "\n",
    "print(\"For aggregator 5:\")\n",
    "print(f\"Weight for u3          - ground truth 0.4 - learned value {cweights5[0,0]}\")\n",
    "print(f\"Weight for u4          - ground truth 0.1 - learned value {cweights5[0,1]}\")\n",
    "print(f\"Weight for min(u3, u4) - ground truth 0.5 - learned value {cweights5[0,2]}\")\n",
    "print(f\"Weight for max(u3, u4) - ground truth 0.0 - learned value {cweights5[0,3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the same way, recall that:\n",
    "$u_{-1} = 0.2 u_0 + 0.3 u_2 + 0.2 u_5 + 0.2 min(u_0, u_5) + 0.1 max(u_1, u_2)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cweightsm1 = model.HCI.CIs['-1'].canonical_weights()\n",
    "\n",
    "print(\"For aggregator -1:\")\n",
    "print(f\"Weight for u0          - ground truth 0.2 - learned value {cweightsm1[0,0]}\")\n",
    "print(f\"Weight for u1          - ground truth 0.0 - learned value {cweightsm1[0,1]}\")\n",
    "print(f\"Weight for u2          - ground truth 0.3 - learned value {cweightsm1[0,2]}\")\n",
    "print(f\"Weight for u5          - ground truth 0.2 - learned value {cweightsm1[0,3]}\")\n",
    "print(f\"Weight for min(u0, u1) - ground truth 0.0 - learned value {cweightsm1[0,4]}\")\n",
    "print(f\"Weight for min(u0, u2) - ground truth 0.0 - learned value {cweightsm1[0,5]}\")\n",
    "print(f\"Weight for min(u0, u5) - ground truth 0.2 - learned value {cweightsm1[0,6]}\")\n",
    "print(f\"Weight for min(u1, u2) - ground truth 0.0 - learned value {cweightsm1[0,7]}\")\n",
    "print(f\"Weight for min(u1, x5) - ground truth 0.0 - learned value {cweightsm1[0,8]}\")\n",
    "print(f\"Weight for min(u2, x5) - ground truth 0.0 - learned value {cweightsm1[0,9]}\")\n",
    "print(f\"Weight for max(u0, x1) - ground truth 0.0 - learned value {cweightsm1[0,10]}\")\n",
    "print(f\"Weight for max(u0, x2) - ground truth 0.0 - learned value {cweightsm1[0,11]}\")\n",
    "print(f\"Weight for max(u0, x5) - ground truth 0.0 - learned value {cweightsm1[0,12]}\")\n",
    "print(f\"Weight for max(u1, x2) - ground truth 0.1 - learned value {cweightsm1[0,13]}\")\n",
    "print(f\"Weight for max(u1, x5) - ground truth 0.0 - learned value {cweightsm1[0,14]}\")\n",
    "print(f\"Weight for max(u2, x5) - ground truth 0.0 - learned value {cweightsm1[0,15]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Möbius transforms\n",
    "\n",
    "The Mobius transform of the parameters of a given aggregator can be obtained through the mobius() method.\n",
    "The ordering is the same as above: first the n singletons, then the $n(n-1)/2$ pairs in lexicographic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mob5 = model.HCI.CIs['5'].mobius()\n",
    "print(f\"\"\"Möbius values for aggregator 5: {mob5}.\n",
    "      The synergy between 3 and 4 is observed by the positive value\n",
    "        of the term corresponding to their min: {mob5[-1]}.\n",
    "      \"\"\")\n",
    "\n",
    "mobm1 = model.HCI.CIs['-1'].mobius()\n",
    "print(f\"\"\"Möbius values for aggregator -1: \n",
    "      {mobm1}.\n",
    "    The synergy between 0 and 5 is observed by the positive value\n",
    "        of the term corresponding to their pair: {mobm1[6]}.\n",
    "    The redundancy between 1 and 2 is observed by the negative value\n",
    "        of the term corresponding to their pair: {mobm1[7]}.\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanations at node level (Shapley values)\n",
    "\n",
    "Finally, we show how to obtain the importance indices for the nodes (i.e. Shapley and Winter values), in both a global and an instance-wise context.\n",
    "\n",
    "The global Shapley values give us the average local relative contribution of each child of a given aggregator wrt the value of said aggregator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap5 = model.HCI.CIs['5'].shapley_values_global()\n",
    "print(\"Children of 5 and their global Shapley values wrt 5:\")\n",
    "for c,s in zip(model.HCI.hierarchy[5], shap5):\n",
    "    print(f\"{c} has a relative global influence of {s}\")\n",
    "print()\n",
    "shapm1 = model.HCI.CIs['-1'].shapley_values_global()\n",
    "print(\"Children of -1 and their global Shapley values wrt -1:\")\n",
    "for c,s in zip(model.HCI.hierarchy[-1], shapm1):\n",
    "    print(f\"{c} has a relative global influence of {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation at model level (Winter values)\n",
    "\n",
    "To obtain the global influence of a leaf wrt the root (or, if needed, of any node wrt to any of its ancestors), we compute the global Winter value (i.e. the generalization of the global Shapley values to a structured model).\n",
    "\n",
    "Note that the model induced by the descendents of the ancestor node MUST be a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_of_leaves = model.HCI.winter_values_global() # If no argument is passed, the root is explained.\n",
    "for l,w in influence_of_leaves.items():\n",
    "    print(f\"{l} has a relative global influence of {w} on the root -1.\")\n",
    "print(\"Note that each non-leaf node's influence is the sum of the influence of its children.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain for a specific node, we use the \"starting_node\" argument. Note that since it is a relative influence, the starting_node's influence on itself will always be 100%. This is akin to only observing the HCI induced by the starting node's descendents, ignoring all other parts of the bigger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "influence_on_5 = model.HCI.winter_values_global(starting_node=5)\n",
    "for l,w in influence_on_5.items():\n",
    "    print(f\"{l} has a relative global influence of {w} on the node 5.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance-wise explanation\n",
    "\n",
    "We now demonstrate how to obtain the instance-wise influence of a node on one of its ancestor. That is, rather than \"how important is the node on average\", we focus on \"how important is that node to explain the difference between two predictions\".\n",
    "\n",
    "This contribution can be positive (the node aims at increasing the difference) or negative. 0. means that the node does not contribute to the difference.\n",
    "\n",
    "To do that, we use the Winter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3, 5)\n",
    "Y = torch.rand(3, 5)\n",
    "# The comparisons will be made linewise, i.e. x[0] vs y[0], x[1] vs y[1]...\n",
    "# In this case, there will be 3 comparisons\n",
    "\n",
    "winter_values = model.winter_values(X, Y) #since the starting node is not given, the root will be used.\n",
    "for i,(x,y) in enumerate(zip(X,Y)):\n",
    "    predx, predy = model(x.unsqueeze(0)), model(y.unsqueeze(0))\n",
    "    print(f\"x = {x}, with prediction {predx.item()}\")\n",
    "    print(f\"y = {y}, with prediction {predy.item()}\")\n",
    "    print(f\"The difference between predictions is {predy.item()-predx.item()}\")\n",
    "    print(f\"The share of each leaf in the difference in prediction is:\")\n",
    "    for k,v in winter_values.items():\n",
    "        print(f\"    Leaf {k} contributes for {v[i].item()} to the difference\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Winter value of a single node wrt another can also be computed. Note that this is useful to look at internal relations among nodes.\n",
    "Note that the importance of a node i wrt to another node j is always equal to the sum of the importances of the leaves descendent of node i wrt node j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_value_5 = model.winter_value_single_node(X, Y, 5, -1) #importance of node 5 wrt node -1\n",
    "winter_values_3p4 = winter_values[3]+winter_values[4]\n",
    "for i in range(3):\n",
    "    print(f'Contributions of node 5 to node -1 for example {i+1}:')\n",
    "    print(winter_value_5[i].item())\n",
    "    print(f'Sum of contributions of nodes 3 and 4 to node -1 for example {i+1}:')\n",
    "    print(winter_values_3p4[i].item())\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
